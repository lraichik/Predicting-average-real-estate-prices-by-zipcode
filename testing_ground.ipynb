{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview  \n",
    "\n",
    "In this project I am working with data from Zillow to try to predict sales prices in the future. The goal is to see which zipcodes would make the most sense to invest in with regards to profit and return on investment.  \n",
    "\n",
    "I will look at all three of these parameters when making my recommendations for these reasons: \n",
    "  \n",
    "**Profit** - Tells us in dollar amount how much we estimate can be made as well as confidence intervels with a min and max profit margin.  \n",
    "  \n",
    "**Return on investment** - Percentage that can be gained, this metric can tell us if the amount we are able to make is a small or big precentage. This will help with understanding that even if a big profit can be made, this may be more of a risk with regards to small market swings. \n",
    "\n",
    "  \n",
    "### City  \n",
    "\n",
    "I will be looking at zipcodes in the city of Chicago, of which there are 41 zipcodes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod 4 Project - Starter Notebook\n",
    "\n",
    "This notebook has been provided to you so that you can make use of the following starter code to help with the trickier parts of preprocessing the Zillow dataset. \n",
    "\n",
    "The notebook contains a rough outline the general order you'll likely want to take in this project. You'll notice that most of the areas are left blank. This is so that it's more obvious exactly when you should make use of the starter code provided for preprocessing. \n",
    "\n",
    "**_NOTE:_** The number of empty cells are not meant to infer how much or how little code should be involved in any given step--we've just provided a few for your convenience. Add, delete, and change things around in this notebook as needed!\n",
    "\n",
    "# Some Notes Before Starting\n",
    "\n",
    "This project will be one of the more challenging projects you complete in this program. This is because working with Time Series data is a bit different than working with regular datasets. In order to make this a bit less frustrating and help you understand what you need to do (and when you need to do it), we'll quickly review the dataset formats that you'll encounter in this project. \n",
    "\n",
    "## Wide Format vs Long Format\n",
    "\n",
    "If you take a look at the format of the data in `zillow_data.csv`, you'll notice that the actual Time Series values are stored as separate columns. Here's a sample: \n",
    "\n",
    "<img src='~/../images/df_head.png'>\n",
    "\n",
    "You'll notice that the first seven columns look like any other dataset you're used to working with. However, column 8 refers to the median housing sales values for April 1996, column 9 for May 1996, and so on. This This is called **_Wide Format_**, and it makes the dataframe intuitive and easy to read. However, there are problems with this format when it comes to actually learning from the data, because the data only makes sense if you know the name of the column that the data can be found it. Since column names are metadata, our algorithms will miss out on what dates each value is for. This means that before we pass this data to our ARIMA model, we'll need to reshape our dataset to **_Long Format_**. Reshaped into long format, the dataframe above would now look like:\n",
    "\n",
    "<img src='~/../images/melted1.png'>\n",
    "\n",
    "There are now many more rows in this dataset--one for each unique time and zipcode combination in the data! Once our dataset is in this format, we'll be able to train an ARIMA model on it. The method used to convert from Wide to Long is `pd.melt()`, and it is common to refer to our dataset as 'melted' after the transition to denote that it is in long format. \n",
    "\n",
    "# Helper Functions Provided\n",
    "\n",
    "Melting a dataset can be tricky if you've never done it before, so you'll see that we have provided a sample function, `melt_data()`, to help you with this step below. Also provided is:\n",
    "\n",
    "* `get_datetimes()`, a function to deal with converting the column values for datetimes as a pandas series of datetime objects\n",
    "* Some good parameters for matplotlib to help make your visualizations more readable. \n",
    "\n",
    "Good luck!\n",
    "\n",
    "\n",
    "# Step 1: Load the Data/Filtering for Chosen Zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('zillow_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[df['City'] == 'Chicago']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking to see where those null values are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing where this zipcode actually starts, as I will need to filter for that in the modeling later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df60611 =  df1.loc[df1['RegionName'] == 60611]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df60611.iloc[:, 214:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Reshape from Wide to Long Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_data(df):\n",
    "    melted = pd.melt(df, id_vars=['RegionID', 'RegionName', 'City', 'State', 'Metro', 'CountyName', 'SizeRank'], var_name='time')\n",
    "    melted['time'] = pd.to_datetime(melted['time'], infer_datetime_format=True)\n",
    "    melted = melted.dropna(subset=['value'])\n",
    "    return melted.groupby('time').aggregate({'value':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_df(df):\n",
    "    merged = []\n",
    "    for zipcode in df.RegionName:\n",
    "        melted = melt_data(df.loc[df['RegionName'] == zipcode])\n",
    "        row = df.loc[df['RegionName'] == zipcode].iloc[:,:7]\n",
    "        rows = pd.concat([row]*len(melted), ignore_index=True)\n",
    "        merge = pd.concat([rows, melted.reset_index()], axis= 1)\n",
    "        merged.append(merge)\n",
    "    melted_df = pd.concat(merged)\n",
    "    return melted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df1 = melt_df(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(melted_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the time column as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df1.set_index('time', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: EDA and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# NOTE: if your visualizations are too cluttered to read, try calling 'plt.gcf().autofmt_xdate()'!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a general visual, just to see the general trend of all the zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "for zipcode in melted_df1.RegionName.unique():\n",
    "    melted_df1.loc[melted_df1['RegionName'] == zipcode].value.plot(label=zipcode)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to checkout the upper part of this graph as the lower part seems to not have much change from the start to the end. As it seems that the ones that have the most change start higher as well, I will just filter for the 10 best means and that should give me what I am looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips = pd.DataFrame()\n",
    "zipsdict = {}\n",
    "for zipcode in melted_df1.RegionName.unique():\n",
    "    zipsdict[zipcode] = melted_df1.loc[melted_df1['RegionName'] == zipcode].value.mean()\n",
    "zips['zipcode'] = zipsdict.keys()\n",
    "zips['means'] = zipsdict.values()\n",
    "highmean_list = list(zips.sort_values(by='means', ascending=False).zipcode[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "for zipcode in highmean_list:\n",
    "    zipdf = melted_df1.loc[melted_df1['RegionName'] == zipcode].value\n",
    "    zipdf.plot(label=zipcode)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems I got what I am looking for, from 1996 until 2019, these seem to be the most growing zipcodes, though what effect that will have on future growth remains to be seen. Might be interesting to check back at this after I figure out the best ones by predictionto see if they match up or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just want to see a basic breakdown of one zipcode to see the seasonality and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = seasonal_decompose(melted_df1.loc[melted_df1['RegionName'] == 60654].value)\n",
    "decomposition.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to be very seasonal and the residuals seem to be more volatile from the start of the corresponding drop in the main graph, which seems to relate to the market crash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: ARIMA Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a test on one zipcode to then use the code for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = melted_df1.loc[melted_df1['RegionName'] == 60654]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA_MODEL = sm.tsa.statespace.SARIMAX(test.value,\n",
    "                                order=(0, 1, 1),\n",
    "                                seasonal_order=(0, 1, 1, 12), \n",
    "                                enforce_stationarity=False, \n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "# Fit the model and print results\n",
    "output = ARIMA_MODEL.fit()\n",
    "display(output.summary().tables[0], output.summary().tables[1], output.summary().tables[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the diagnostics of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.plot_diagnostics(figsize=(15, 18))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking getting the predictions along with the lower and upper confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = output.get_prediction(start=pd.to_datetime('2015-01-01'), dynamic=False)\n",
    "pred_conf = pred.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "b['a'] = pred_conf[pred_conf.index == '2015-02-01']['lower value'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_conf[pred_conf.index == '2015-02-01']['lower value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_conf['upper value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.predicted_mean['2018-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a graph of the predictions and the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 15, 6\n",
    "\n",
    "#Plot observed values\n",
    "ax = test['2014':].value.plot(label='observed')\n",
    "\n",
    "#Plot predicted values\n",
    "pred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7)\n",
    "\n",
    "#Plot the range for confidence intervals\n",
    "ax.fill_between(pred_conf.index,\n",
    "                pred_conf.iloc[:, 0],\n",
    "                pred_conf.iloc[:, 1], color='y', alpha=.5)\n",
    "\n",
    "#Set axes labels\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Values')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forecasted = pred.predicted_mean\n",
    "test_truth = test['2017':].value\n",
    "test_forecasted = test_forecasted['2017':]\n",
    "# Compute the root mean square error\n",
    "error = mean_squared_error(test_forecasted, test_truth)\n",
    "error = np.sqrt(error)\n",
    "print(f'The RMSE of our forecasts is {error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictions are within about 380,000 dollars. Meaning about give or take 140,000 dollars. This is not too bad concidering that this zipcode has houses over a million dollars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to figure out the best parameters to use for the model, based on AIC.  \n",
    "First I will generate the possible combinations and then get the best one for each zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the p, d and q parameters to take any value between 0 and 2\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "pdqs = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combs = []\n",
    "for zipcode in tqdm(melted_df1.RegionName.unique()):\n",
    "    ans = []\n",
    "    for comb in pdq:\n",
    "        for combs in pdqs:\n",
    "            try:\n",
    "                mod = sm.tsa.statespace.SARIMAX(melted_df1.loc[melted_df1['RegionName'] == zipcode].value,\n",
    "                                                order=comb,\n",
    "                                                seasonal_order=combs,\n",
    "                                                enforce_stationarity=False,\n",
    "                                                enforce_invertibility=False)\n",
    "\n",
    "                output = mod.fit()\n",
    "                ans.append([zipcode, comb, combs, output.aic])\n",
    "    #             print(f'ARIMA {comb} x {combs}12 : AIC Calculated ={output.aic}')\n",
    "            except:\n",
    "                continue\n",
    "    best_combs.append(sorted(ans, key=lambda x: x[3])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the best combinations for each zipcode, using 12 for seasonality and a 0 or a 1 for everything else.  \n",
    "I will now try to check changing everything, including seasonality to see which ones are the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the p, d and q parameters to take any value between 0 and 10\n",
    "p = d = q = range(0, 11)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Define the p, d, q and s parameters to take any value between 0 and 10\n",
    "p2 = d2 = q2 = s = range(0, 11)\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "pdqs = [(x[0], x[1], x[2], x[3]) for x in list(itertools.product(p2, d2, q2, s))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function for creating an image with the original data, and the predicted data, along with the forcasted data and the confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred_image(df, test_forecasted, forcast, zipcode=None):\n",
    "    rcParams['figure.figsize'] = 15, 6\n",
    "\n",
    "    #Plot observed values\n",
    "    ax = df['2015':].plot(label='observed')\n",
    "\n",
    "    #Plot predicted values\n",
    "    test_forecasted['2015-01-01':].plot(ax=ax, label='Predicted', alpha=.7)\n",
    "    \n",
    "    forcast.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7)\n",
    "\n",
    "    #Plot the range for confidence intervals\n",
    "    ax.fill_between(pred_conf.index,\n",
    "                    pred_conf.iloc[:, 0],\n",
    "                    pred_conf.iloc[:, 1], color='y', alpha=.5)\n",
    "\n",
    "    #Set axes labels\n",
    "#     ax.title(zipcode)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Values')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using a loop to get predictions for each zipcode based on the best combinations of parameters.  \n",
    "It will also print out a graph for each zipcode along with the diagnostics.  \n",
    "I put all the metrics I want into dictionaries with the zipcode as the key and the metric as the value.  \n",
    "I also setup a dataframe and then make each dictionary correspond to a column in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "known_RMSE = {}\n",
    "forcast_2019_04 = {}\n",
    "confid_min = {}\n",
    "confid_max = {}\n",
    "profit = {}\n",
    "roi = {}\n",
    "for comb in best_combs:\n",
    "    \n",
    "    if comb[0] == 60611:\n",
    "        \n",
    "        df = melted_df1.loc[melted_df1['RegionName'] == comb[0]].value.dropna()\n",
    "        \n",
    "        mod = sm.tsa.statespace.SARIMAX(df,\n",
    "                                    order=comb[1],\n",
    "                                    seasonal_order=comb[2],\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False,)\n",
    "\n",
    "        output = mod.fit()\n",
    "        pred = output.get_prediction(start=pd.to_datetime('2013-07-01'), dynamic=False)\n",
    "        test_forecasted = pred.predicted_mean\n",
    "        test_truth = test['2013-07-01':].value\n",
    "        test_forecasted = test_forecasted['2013-07-01':]\n",
    "    else:\n",
    "        \n",
    "        df = melted_df1.loc[melted_df1['RegionName'] == comb[0]].value.dropna()\n",
    "       \n",
    "        mod = sm.tsa.statespace.SARIMAX(df,\n",
    "                                    order=comb[1],\n",
    "                                    seasonal_order=comb[2],\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False)\n",
    "\n",
    "        output = mod.fit()\n",
    "        pred = output.get_prediction(start=pd.to_datetime('1996-04-01'), dynamic=False)\n",
    "        test_forecasted = pred.predicted_mean\n",
    "        test_truth = test['1996-04-01':].value\n",
    "        test_forecasted = test_forecasted['1996-04-01':]\n",
    "    error = mean_squared_error(test_forecasted, test_truth)\n",
    "    known_RMSE[comb[0]] = np.sqrt(error)\n",
    "    forcast = output.get_forecast(steps=24)\n",
    "    forcast_2019_04[comb[0]] = forcast.predicted_mean['2019-04-01']\n",
    "    pred_conf = forcast.conf_int()\n",
    "    confid_min[comb[0]] = pred_conf[pred_conf.index == '2019-04-01']['lower value'][0]\n",
    "    confid_max[comb[0]] = pred_conf[pred_conf.index == '2019-04-01']['upper value'][0]\n",
    "    val_04_18 = melted_df1.loc[melted_df1['RegionName'] == comb[0]].value[-1]\n",
    "    profit[comb[0]] = forcast.predicted_mean['2019-04'] - val_04_18\n",
    "    roi[comb[0]] = profit[comb[0]]/val_04_18\n",
    "    \n",
    "    print(comb[0])\n",
    "    make_pred_image(df, test_forecasted, forcast)\n",
    "    output.plot_diagnostics(figsize=(12, 15))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "pred_df['zipcode'] = known_RMSE.keys()\n",
    "pred_df['known_RMSE'] = known_RMSE.values()\n",
    "pred_df['forcast_2019_04'] = forcast_2019_04.values()\n",
    "pred_df['confid_min'] = confid_min.values()\n",
    "pred_df['confid_max'] = confid_max.values()\n",
    "pred_df['profit'] = profit.values()\n",
    "pred_df['roi'] = roi.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up how the data is displayed to make it easier to read in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['roi'] = pred_df['roi'].apply(lambda x: round(x[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['profit'] = pred_df['profit'].apply(lambda x: round(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['2018_04_price'] = pred_df['forcast_2019_04'] - pred_df['profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['2018_04_price'] = pred_df['2018_04_price'].apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['forcast_2019_04'] = pred_df['forcast_2019_04'].apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['known_RMSE'] = pred_df['known_RMSE'].apply(lambda x: round(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['confid_min'] = pred_df['confid_min'].apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['confid_max'] = pred_df['confid_max'].apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.sort_values(by=['profit', 'roi'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_df_d = pd.DataFrame()\n",
    "known_RMSE = {}\n",
    "forcast_2019_04 = {}\n",
    "confid_min = {}\n",
    "confid_max = {}\n",
    "profit = {}\n",
    "roi = {}\n",
    "for comb in best_combs:\n",
    "    \n",
    "    if comb[0] == 60611:\n",
    "        \n",
    "        df = melted_df1.loc[melted_df1['RegionName'] == comb[0]].value.dropna()\n",
    "        \n",
    "        mod = sm.tsa.statespace.SARIMAX(df,\n",
    "                                    order=comb[1],\n",
    "                                    seasonal_order=comb[2],\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False,)\n",
    "\n",
    "        output = mod.fit()\n",
    "        pred = output.get_prediction(start=pd.to_datetime('2013-07-01'), dynamic=True)\n",
    "        test_forecasted = pred.predicted_mean\n",
    "        test_truth = test['2013-07-01':].value\n",
    "        test_forecasted = test_forecasted['2013-07-01':]\n",
    "    else:\n",
    "        \n",
    "        df = melted_df1.loc[melted_df1['RegionName'] == comb[0]].value.dropna()\n",
    "       \n",
    "        mod = sm.tsa.statespace.SARIMAX(df,\n",
    "                                    order=comb[1],\n",
    "                                    seasonal_order=comb[2],\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False)\n",
    "\n",
    "        output = mod.fit()\n",
    "        pred = output.get_prediction(start=pd.to_datetime('1996-04-01'), dynamic=True, full_results=True)\n",
    "        test_forecasted = pred.predicted_mean\n",
    "        test_truth = test['1996-04-01':].value\n",
    "        test_forecasted = test_forecasted['1996-04-01':]\n",
    "    error = mean_squared_error(test_forecasted, test_truth)\n",
    "    known_RMSE[comb[0]] = np.sqrt(error)\n",
    "    forcast = output.get_forecast(steps=24, dynamic=True)\n",
    "    forcast_2019_04[comb[0]] = forcast.predicted_mean['2019-04-01']\n",
    "    pred_conf = forcast.conf_int()\n",
    "    confid_min[comb[0]] = pred_conf[pred_conf.index == '2019-04-01']['lower value'][0]\n",
    "    confid_max[comb[0]] = pred_conf[pred_conf.index == '2019-04-01']['upper value'][0]\n",
    "    val_04_18 = melted_df1.loc[melted_df1['RegionName'] == comb[0]].value[-1]\n",
    "    profit[comb[0]] = forcast.predicted_mean['2019-04'] - val_04_18\n",
    "    roi[comb[0]] = profit[comb[0]]/val_04_18\n",
    "    \n",
    "    print(comb[0])\n",
    "    make_pred_image(df, test_forecasted, forcast)\n",
    "    output.plot_diagnostics(figsize=(12, 15))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "pred_df_d['zipcode'] = known_RMSE.keys()\n",
    "pred_df_d['known_RMSE'] = known_RMSE.values()\n",
    "pred_df_d['forcast_2019_04'] = forcast_2019_04.values()\n",
    "pred_df_d['confid_min'] = confid_min.values()\n",
    "pred_df_d['confid_max'] = confid_max.values()\n",
    "pred_df_d['profit'] = profit.values()\n",
    "pred_df_d['roi'] = roi.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_d['roi'] = pred_df_d['roi'].apply(lambda x: round(x[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_d['profit'] = pred_df_d['profit'].apply(lambda x: round(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_d['2018_04_price'] = pred_df_d['forcast_2019_04'] - pred_df['profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_d['2018_04_price'] = pred_df_d['2018_04_price'].apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_d['forcast_2019_04'] = pred_df_d['forcast_2019_04'].apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_d['known_RMSE'] = pred_df_d['known_RMSE'].apply(lambda x: round(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_d['confid_min'] = pred_df_d['confid_min'].apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_d['confid_max'] = pred_df_d['confid_max'].apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_d.sort_values(by=['profit', 'roi'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.sort_values(by=['profit', 'roi'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Interpreting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
